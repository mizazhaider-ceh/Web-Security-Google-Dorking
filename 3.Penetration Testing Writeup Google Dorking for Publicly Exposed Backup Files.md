# Penetration Testing Writeup: Google Dorking for Publicly Exposed Backup Files

> **Writeup Classification**: This is a standard penetration testing writeup documenting the discovery of publicly accessible backup files through OSINT reconnaissance. Writeups serve as critical documentation for demonstrating attack chains, from initial discovery through credential extraction to potential system compromise.

***

## üìã Engagement Overview

| **Test Type** | OSINT / Reconnaissance - Backup File Enumeration |
| :-- | :-- |
| **Methodology** | Google Dorking for File Extensions |
| **Target Domain** | target.com |
| **Test Date** | 2025-12-27 |
| **Vulnerability Type** | Sensitive Data Exposure via Backup Files |
| **Severity** | Critical (Database Dump with Password Hashes) |


***

## üéØ Executive Summary

During systematic OSINT reconnaissance operations, Google Dorking techniques successfully identified a publicly accessible database backup file indexed by Google. The discovered `.backup` file contained a complete SQL database dump including administrative credentials in the form of password hashes. This represents a critical sensitive data exposure vulnerability with immediate exploitation potential through offline password cracking.

**Key Finding**: Database backup file (`database.backup`) containing admin email and password hash exposed to public internet.

**Attack Path**: OSINT Discovery ‚Üí File Download ‚Üí Hash Extraction ‚Üí Password Cracking ‚Üí Administrative Access.

***

## üîç Vulnerability Background: Backup File Exposure

### What Are Backup Files?

Backup files are copies of application data, databases, source code, or configuration files created for disaster recovery, version control, or administrative purposes. Common scenarios that create backup files include:

1. **Automated Backup Systems**: Scheduled database dumps, application snapshots
2. **Manual Administrator Backups**: Ad-hoc copies before maintenance or updates
3. **Editor Auto-Saves**: Text editors creating `.bak`, `.old`, or `~` suffixed copies
4. **Version Control Accidents**: Exposed `.git`, `.svn` directories with full history
5. **Development Artifacts**: Test files, old versions left in production
6. **Migration Remnants**: Legacy files from platform migrations or refactoring

### Why Backup Files Are High-Value Targets

Backup files are extremely dangerous when publicly accessible because they contain:

1. **Complete Application Source Code**: Enables code review for vulnerabilities
2. **Database Dumps**: Full datasets including user credentials, PII, financial data
3. **Configuration Files**: Database connection strings, API keys, encryption keys
4. **Historical Vulnerabilities**: Old code versions may contain patched bugs still exploitable in backups
5. **Internal Architecture**: File structure reveals application design and attack surface
6. **Unencrypted Credentials**: Password hashes without salting or with weak algorithms

### OWASP Classification

- **Category**: A01:2021 ‚Äì Broken Access Control / A02:2021 ‚Äì Cryptographic Failures
- **CWE-530**: Exposure of Backup File to an Unauthorized Control Sphere
- **CWE-312**: Cleartext Storage of Sensitive Information
- **CWE-538**: Insertion of Sensitive Information into Externally-Accessible File or Directory

***

## üì¶ Common Backup File Extensions and Patterns

### Standard Backup Extensions

Understanding backup file naming conventions is critical for effective Google Dorking. Different systems and administrators use predictable patterns:

#### 1. Generic Backup Extensions

| **Extension** | **Common Usage** | **Typical Contents** |
| :-- | :-- | :-- |
| `.backup` | Generic backup suffix  | Databases, configuration files, full application backups |
| `.bak` | Windows/DOS convention  | Any file type backup (database.bak, web.config.bak) |
| `.old` | Legacy version marker  | Deprecated files, old configurations, previous code versions |
| `.orig` | Original file preservation | Pre-modification copies, configuration originals |
| `.copy` | Manual copy indicator | Administrative duplicates |
| `.save` | Save point marker | Pre-change snapshots |

#### 2. Database-Specific Backup Extensions

```
# MySQL/MariaDB
.sql
.mysql
.dump
.sql.gz
.sql.bak

# PostgreSQL
.pgsql
.psql
.pg_dump
.pgdump

# MongoDB
.bson
.json (MongoDB exports)
.archive

# Microsoft SQL Server
.mdf (database file)
.bak (SQL Server backup)
.trn (transaction log backup)
```


#### 3. Archive and Compression Formats

```
# Compressed archives
.zip
.tar
.tar.gz / .tgz
.tar.bz2
.7z
.rar

# Version-specific
.tar.2024-12-27
.backup-20241227
database-v1.zip
site-backup-final.tar.gz
```


#### 4. Editor Auto-Backup Patterns

```
# Vim/Vi
file~
file.swp
.file.swo

# Emacs
file~
#file#

# Sublime Text
file.sublime-workspace
file.sublime-project

# Visual Studio
file.bak
file.old

# macOS
._file
file (1)
file copy
```


#### 5. Version Control Exposures

```
# Git
.git/
.gitignore
.git/config (contains repo URL)
.git/HEAD

# Subversion (SVN)
.svn/
.svn/entries

# Mercurial
.hg/
```


***

## üé¨ Step-by-Step Attack Execution

### Phase 1: Query Construction and Strategy

**Objective**: Discover publicly indexed backup files on target domain

**Query Design Rationale**:

- Target specific domain with `site:` operator
- Use `ext:` operator to search for specific file extensions
- Combine multiple extensions with `OR` logic to maximize coverage
- Focus on most common backup extensions first (backup, bak, old)

**Primary Query**:

```
site:target.com AND (ext:"backup" OR ext:"bak" OR ext:"old")
```

**Query Component Breakdown**:


| **Component** | **Function** | **Pentester Value** |
| :-- | :-- | :-- |
| `site:target.com` | Scope limitation  | Ensures results are within authorized testing scope |
| `AND` | Boolean connector  | Requires both domain match and file extension match |
| `ext:"backup"` | File extension filter  | Finds files ending in `.backup` |
| `ext:"bak"` | File extension filter  | Finds files ending in `.bak` |
| `ext:"old"` | File extension filter  | Finds files ending in `.old` |
| `OR` | Logical disjunction  | Returns results matching any of the extension criteria |

### Phase 2: Google Search Execution

**Action**: Execute constructed query in Google Search interface

**Command**:

```
site:target.com AND (ext:"backup" OR ext:"bak" OR ext:"old")
```

**Result**: One indexed file discovered with `.backup` extension

**Google Search Results Page Indicators**:

- File URL: `https://target.com/backup/database.backup`
- File description from meta tags or content snippets
- Last indexed date by Google crawler
- File size (if available in search snippet)

**Pentester Analysis**: The presence of a `/backup/` directory in the URL path is a critical indicator:

- **Directory Structure**: Suggests organized backup storage location
- **Naming Convention**: `database.backup` clearly indicates database content
- **Public Accessibility**: Google indexing confirms no authentication required
- **Forgotten Asset**: Likely an administrative oversight or misconfiguration


### Phase 3: File Download and Acquisition

**Action**: Download discovered backup file for analysis

**Download Method**:

```bash
# Direct download via wget
wget https://target.com/backup/database.backup

# Alternative: curl
curl -O https://target.com/backup/database.backup

# Verify file integrity
md5sum database.backup
file database.backup  # Identify actual file type
```

**File Analysis Output**:

```
database.backup: ASCII text, with very long lines
```

**Pentester Insight**: The file is ASCII text, suggesting either:

- Plain SQL dump (uncompressed)
- JSON export
- CSV data export
- XML backup

The lack of compression indicates potential rush/carelessness in backup creation.

### Phase 4: Content Analysis and Format Identification

**Initial Inspection**:

```bash
# View file type and initial content
head -n 20 database.backup

# Search for SQL keywords
grep -i "INSERT INTO" database.backup | head
grep -i "CREATE TABLE" database.backup | head
grep -i "password" database.backup
```

**Discovery**: File confirmed as SQL database dump

**Format Change for Analysis**:

```
# Change display format from plain text to SQL syntax highlighting
# Using text editor with SQL syntax support (VS Code, Sublime Text, vim with SQL plugin)
```

**SQL Dump Structure Observed**:

```sql
-- Database backup file
-- Generated: 2024-12-15 14:32:11
-- Server: MySQL 8.0.35

CREATE TABLE IF NOT EXISTS users (
    id INT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(255) NOT NULL,
    email VARCHAR(255) NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    role ENUM('user', 'admin') DEFAULT 'user',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

INSERT INTO users (id, username, email, password_hash, role) VALUES
(1, 'john_doe', 'john@example.com', '$2y$10$abcd1234...', 'user'),
(2, 'admin', 'admin@target.com', '$2y$10$xyz7890...', 'admin'),
...
```


### Phase 5: Critical Finding - Administrative Credential Extraction

**Discovered Data**:

**Admin Email**: `admin@target.com`
**Password Hash**: `$2y$10$92IXUNpkjO0rOQ5byMi.Ye4oKoEa3Ro9llC/.og/at2.uheWG/igi`

**Hash Analysis**:

```bash
# Identify hash type
hashid '$2y$10$92IXUNpkjO0rOQ5byMi.Ye4oKoEa3Ro9llC/.og/at2.uheWG/igi'

# Output:
Analyzing '$2y$10$92IXUNpkjO0rOQ5byMi.Ye4oKoEa3Ro9llC/.og/at2.uheWG/igi'
[+] Blowfish(OpenBSD) 
[+] Woltlab Burning Board 4.x 
[+] bcrypt
```

**Hash Format Breakdown**:

- **Algorithm**: bcrypt
- **Cost Factor**: 10 (2^10 = 1,024 iterations)
- **Salt**: `92IXUNpkjO0rOQ5byMi.Ye` (bcrypt includes salt in hash)
- **Hash**: `4oKoEa3Ro9llC/.og/at2.uheWG/igi`

**Security Assessment**:

- **Algorithm Strength**: bcrypt is a strong password hashing algorithm
- **Cost Factor**: 10 is moderate; modern recommendations suggest 12+ for high security
- **Exploitability**: Crackable with sufficient computational resources and good wordlists

***

## üîì Phase 6: Password Cracking with Hashcat

### Hashcat Overview

Hashcat is the world's fastest and most advanced password recovery tool. It supports over 300 hash types and utilizes CPU, GPU, and hybrid processing for maximum speed.

**Why Hashcat?**

- **Performance**: GPU acceleration provides orders of magnitude speed increase
- **Flexibility**: Supports dictionary attacks, brute-force, mask attacks, hybrid attacks
- **Rule-Based Attacks**: Applies transformation rules to wordlists for better coverage
- **Distributed Cracking**: Can distribute workload across multiple machines


### Cracking Methodology: Heath Adams' 5-Step Process

#### Step 1: Quick Wins - Rockyou.txt with Simple Rules

**Attack Strategy**: Use the most common wordlist with basic transformation rules for rapid results

```bash
# Hashcat mode 3200 = bcrypt
hashcat -m 3200 -a 0 admin_hash.txt rockyou.txt -r OneRuleToRuleThemAll.rule -O

# Breakdown:
# -m 3200: bcrypt hash mode
# -a 0: Dictionary attack (wordlist)
# -O: Optimized kernel (faster but less compatible)
# admin_hash.txt: File containing the hash
# rockyou.txt: 14 million password wordlist (from 2009 breach)
# -r: Apply transformation rules
```

**Expected Runtime**: 5-15 minutes on modern GPU (RTX 3080/4090)

**Success Scenario**:

```
$2y$10$92IXUNpkjO0rOQ5byMi.Ye4oKoEa3Ro9llC/.og/at2.uheWG/igi:password123

Session..........: hashcat
Status...........: Cracked
Hash.Mode........: 3200 (bcrypt $2*$, Blowfish (Unix))
Time.Started.....: Sat Dec 27 20:15:32 2025
Time.Estimated...: Sat Dec 27 20:23:18 2025 (7 mins, 46 secs)
Recovered........: 1/1 (100.00%) Digests
```

**Pentester Value**: If the administrator used a common password, this step provides immediate access.

#### Step 2: Extended Wordlist - Rockyou2021.txt

**Attack Strategy**: Larger wordlist (8.4 billion passwords) for more comprehensive coverage

```bash
# Rockyou2021 is a compilation of multiple breach datasets
hashcat -m 3200 -a 0 admin_hash.txt rockyou2021.txt -r OneRuleToRuleThemAll.rule -O
```

**Expected Runtime**: 2-6 hours on modern GPU

**Use Case**: When initial quick wins fail but time permits extended cracking

#### Step 3: Custom Wordlist - Target-Specific Dictionary

**Attack Strategy**: Build custom wordlist from target reconnaissance

**Wordlist Sources**:

```bash
# Company name variations
echo "HackingWebApplications" > custom.txt
echo "hacking-web-applications" >> custom.txt
echo "HWA" >> custom.txt

# Domain components
echo "hacking" >> custom.txt
echo "web" >> custom.txt
echo "applications" >> custom.txt

# Common company password patterns
echo "Company2024!" >> custom.txt
echo "Welcome2024" >> custom.txt

# Technology stack keywords (from reconnaissance)
echo "MySQL" >> custom.txt
echo "Apache" >> custom.txt

# Apply rules
hashcat -m 3200 -a 0 admin_hash.txt custom.txt -r dive.rule -O
```

**Rule Example** (dive.rule):

```
# Append current year
$2$0$2$5

# Capitalize first letter
c

# Append common symbols
$! $@ $#

# Leet speak transformations
so0 sl1 se3 sa@
```

**Expected Runtime**: Minutes to 1 hour (small wordlist)

**Pentester Value**: Highly effective for corporate environments with predictable password policies

#### Step 4: Mask Attack - Brute Force Known Patterns

**Attack Strategy**: If password policy is known (e.g., 8 characters minimum), use pattern-based brute force

**Common Masks**:

```bash
# 8 characters: Uppercase, lowercase, digit, special
hashcat -a 3 -m 3200 admin_hash.txt ?u?l?l?l?l?l?d?s -O

# Mask breakdown:
# ?u = Uppercase letter (A-Z)
# ?l = Lowercase letter (a-z)  
# ?d = Digit (0-9)
# ?s = Special character (!@#$%...)

# Password pattern: Admin2024!
hashcat -a 3 -m 3200 admin_hash.txt ?u?l?l?l?l?d?d?d?d?s -O

# Corporate pattern: Winter2024!
hashcat -a 3 -m 3200 admin_hash.txt ?u?l?l?l?l?l?d?d?d?d?s -O
```

**Expected Runtime**: Hours to days depending on complexity

**Optimization**: Use known patterns from password policy or previous findings

#### Step 5: Hybrid Attacks - Combination Approaches

**Attack Strategy**: Combine wordlist with mask attack for passwords like "password2024!"

```bash
# Wordlist + append 4 digits
hashcat -a 6 -m 3200 admin_hash.txt rockyou.txt ?d?d?d?d -O

# Wordlist + append year + special
hashcat -a 6 -m 3200 admin_hash.txt rockyou.txt ?d?d?d?d?s -O

# Prepend digits + wordlist
hashcat -a 7 -m 3200 admin_hash.txt ?d?d?d?d rockyou.txt -O
```

**Real-World Pattern Examples**:

- `Password2024!`
- `Summer2025#`
- `Welcome2024@`
- `Admin123!`

**Expected Runtime**: 1-4 hours

### Hash Cracking Results Analysis

**If Password Cracked**:

```
Original Hash: $2y$10$92IXUNpkjO0rOQ5byMi.Ye4oKoEa3Ro9llC/.og/at2.uheWG/igi
Plaintext Password: Admin2024!
```

**Attack Chain Completion**:

1. ‚úÖ Backup file discovered via Google Dorking
2. ‚úÖ Database dump downloaded and analyzed
3. ‚úÖ Admin credentials extracted (email + hash)
4. ‚úÖ Password hash cracked with Hashcat
5. ‚úÖ Administrative access credentials obtained

**Next Exploitation Steps**:

- Attempt login to application admin panel
- Test credentials against SSH/RDP if applicable
- Check for credential reuse across other services
- Assess privilege level and available administrative functions

***

## üö® Impact Analysis: Pentester Perspective

### Immediate Security Risks

#### 1. Complete User Database Compromise

**Data Exposed**:

- All user accounts (usernames, emails, password hashes)
- User roles and permission levels
- Account metadata (creation dates, last login)
- Personally Identifiable Information (PII)

**Attack Capabilities**:

- Crack all user password hashes offline
- Target high-privilege accounts (admins, moderators)
- Build credential stuffing lists for other services
- Phishing campaigns using real user data


#### 2. Administrative Account Takeover

**With Admin Access**:

- **User Management**: Create, modify, delete any user account
- **Content Control**: Modify application content, inject malicious code
- **Configuration Changes**: Alter security settings, disable protections
- **Data Exfiltration**: Export complete datasets via admin functions
- **Malware Distribution**: Upload malicious files through admin upload features


#### 3. Application Architecture Disclosure

**Database Schema Reveals**:

```sql
CREATE TABLE payment_methods (
    id INT PRIMARY KEY,
    user_id INT,
    card_number_encrypted VARCHAR(255),
    expiry_date VARCHAR(7),
    billing_address TEXT,
    FOREIGN KEY (user_id) REFERENCES users(id)
);
```

**Pentester Intelligence**:

- Database structure and table relationships
- Column names for SQL injection crafting
- Encryption methods used (or lack thereof)
- Business logic and data flow
- Sensitive data storage locations


#### 4. Regulatory Compliance Violations

**GDPR (General Data Protection Regulation)**:

- Article 32: Inadequate security measures for personal data
- Article 33: Data breach notification requirement (72 hours)
- Potential fines: Up to ‚Ç¨20 million or 4% of global annual revenue

**PCI DSS (Payment Card Industry Data Security Standard)**:

- Requirement 3.4: Cardholder data rendering unreadable
- Requirement 8: Strong authentication and password policies
- Potential consequences: Loss of payment processing capabilities

**HIPAA (Health Insurance Portability and Accountability Act)**:

- If health data present: PHI exposure violation
- Civil penalties: \$100 - \$50,000 per violation
- Criminal penalties: Up to \$250,000 and 10 years imprisonment


#### 5. Long-Term Persistent Threats

**Attackers Can**:

- Establish backdoor accounts that blend with legitimate users
- Maintain access even after initial vulnerability is patched
- Return months later using cracked credentials
- Sell database access on dark web marketplaces

***

## üïµÔ∏è What Pentesters Should Look For in Backup Files

### High-Value Data Elements

#### 1. User Credentials

**Priority Targets**:

```sql
SELECT username, email, password_hash, role 
FROM users 
WHERE role IN ('admin', 'superuser', 'moderator');
```

**Information Value**:

- Administrative accounts (highest priority)
- Password hashing algorithm quality assessment
- Salt presence and randomness
- Credential reuse analysis opportunities


#### 2. Configuration Data

**Look For**:

```sql
-- API keys and secrets
SELECT * FROM settings WHERE setting_name LIKE '%api%';

-- External service credentials
SELECT * FROM integrations;

-- Email server configuration
SELECT * FROM mail_settings;
```

**Exploitation Potential**:

- AWS/Azure/GCP cloud access keys
- Payment gateway API credentials (Stripe, PayPal)
- Email service API keys (SendGrid, Mailgun)
- Third-party service integrations


#### 3. Sensitive Business Data

**Data Categories**:

- Financial transactions and payment records
- Customer personal information (names, addresses, phone numbers)
- Business metrics and confidential analytics
- Proprietary algorithms or business logic
- Legal documents and contracts


#### 4. Application Vulnerabilities in Schema

**Red Flags**:

```sql
-- Plaintext password storage (critical finding)
CREATE TABLE old_users (password VARCHAR(255));

-- Weak encryption indicators
CREATE TABLE cards (number VARCHAR(16));  -- No encryption

-- Predictable IDs (IDOR vulnerability potential)
CREATE TABLE documents (id INT AUTO_INCREMENT, user_id INT, filename VARCHAR(255));
```

**Security Issues**:

- SQL injection opportunities from column names
- Business logic flaws visible in schema design
- Insufficient access controls


#### 5. Source Code in Backup Archives

**If Backup Contains Application Files**:

```bash
# Extract and analyze
unzip site-backup.zip
grep -r "password" ./
grep -r "api_key" ./
grep -r "secret" ./
grep -r "TODO" ./  # Developer notes often reveal issues
```

**Code Review Targets**:

- Hardcoded credentials
- Vulnerable library versions
- Authentication logic flaws
- Input validation weaknesses
- Commented-out security controls

***

## üîß Extended Google Dork Arsenal for Backup Files

### Comprehensive Backup File Discovery Queries

#### 1. Extension-Based Searches

```
# Primary backup extensions
site:target.com ext:backup
site:target.com ext:bak
site:target.com ext:old

# Database-specific backups
site:target.com ext:sql
site:target.com ext:dump
site:target.com ext:mysql
site:target.com ext:pgsql

# Archive formats
site:target.com ext:zip inurl:backup
site:target.com ext:tar.gz inurl:backup
site:target.com ext:7z inurl:backup
site:target.com (ext:tar OR ext:tgz) inurl:backup

# Combined multi-extension search
site:target.com (ext:backup OR ext:bak OR ext:old OR ext:sql OR ext:dump)
```


#### 2. Directory Pattern Searches

```
# Backup directory indicators
site:target.com inurl:backup
site:target.com inurl:backups
site:target.com inurl:/old/
site:target.com inurl:/backup/
site:target.com inurl:archive
site:target.com inurl:archives

# Date-stamped backups
site:target.com inurl:backup-2024
site:target.com inurl:backup-2025
site:target.com inurl:20241227
site:target.com inurl:backup_12_27_2024

# Common naming patterns
site:target.com inurl:"backup"
site:target.com intitle:"index of" backup
site:target.com intitle:"index of" "backup"
```


#### 3. Filename Pattern Searches

```
# Database backup naming conventions
site:target.com "database.sql"
site:target.com "db_backup.sql"
site:target.com "mysqldump"
site:target.com "backup.sql"
site:target.com "dump.sql"

# Application backup names
site:target.com "site-backup"
site:target.com "full-backup"
site:target.com "complete-backup"
site:target.com "www-backup"

# Versioned backups
site:target.com "backup-v1"
site:target.com "backup-final"
site:target.com "backup-old"
site:target.com "backup.zip.bak"  # Backup of backup
```


#### 4. Editor Backup Artifacts

```
# Vim/Vi backups
site:target.com ext:swp
site:target.com inurl:"~" (filetype:php OR filetype:asp)

# Common editor patterns
site:target.com "config.php~"
site:target.com "index.php.bak"
site:target.com "web.config.old"
site:target.com ".htaccess.bak"
```


#### 5. Version Control Exposure

```
# Git repository exposure
site:target.com inurl:".git" intitle:"index of"
site:target.com inurl:".git/config"
site:target.com inurl:".git/HEAD"

# SVN exposure
site:target.com inurl:".svn" intitle:"index of"
site:target.com inurl:".svn/entries"

# Mercurial
site:target.com inurl:".hg"
```


#### 6. Cloud Storage Backup Leaks

```
# AWS S3 buckets
site:s3.amazonaws.com "backup"
site:s3.amazonaws.com "target-company"
site:s3.amazonaws.com "database"

# Azure Storage
site:blob.core.windows.net "backup"
site:blob.core.windows.net "target-company"

# Google Cloud Storage
site:storage.googleapis.com "backup"
site:storage.googleapis.com "target-company"
```


#### 7. Specific File Type Combinations

```
# SQL dumps with compression
site:target.com (ext:sql.gz OR ext:sql.zip OR ext:sql.bak)

# Tar archives with backup indicators
site:target.com ext:tar.gz "backup"
site:target.com ext:tar.gz inurl:backup
site:target.com ext:tgz (inurl:backup OR inurl:archive)

# RAR archives (less common but worth checking)
site:target.com ext:rar "backup"

# 7zip compressed backups
site:target.com ext:7z (backup OR archive OR old)
```


***

## üéØ Pentester's Backup File Enumeration Methodology

### Systematic Discovery Process

#### Phase 1: Reconnaissance and Intelligence Gathering

**Subdomain Enumeration**:

```bash
# Discover all subdomains first
amass enum -passive -d target.com -o subdomains.txt
subfinder -d target.com -o subdomains.txt
assetfinder --subs-only target.com >> subdomains.txt

# Backup/dev subdomains are often forgotten
grep -E "(backup|old|archive|dev|staging|test)" subdomains.txt
```

**Common Backup Subdomain Patterns**:

- `backup.target.com`
- `backups.target.com`
- `archive.target.com`
- `old.target.com`
- `legacy.target.com`
- `dev.target.com` (often has backups)
- `staging.target.com`


#### Phase 2: Multi-Extension Dorking

**Automated Query Generation**:

```python
#!/usr/bin/env python3

target = "target.com"
extensions = ["backup", "bak", "old", "sql", "dump", "zip", "tar.gz", "7z"]

for ext in extensions:
    dork = f'site:{target} ext:{ext}'
    print(f"[*] Testing: {dork}")
    # Execute search and process results
```

**Priority Order**:

1. Database extensions (`.sql`, `.dump`, `.mysql`) - Highest value
2. Generic backups (`.backup`, `.bak`, `.old`)
3. Archives (`.zip`, `.tar.gz`, `.7z`)
4. Editor artifacts (`.swp`, `~`)
5. Version control (`.git`, `.svn`)

#### Phase 3: Directory and Naming Pattern Analysis

**Common Backup Directory Structures**:

```
/backup/
/backups/
/backup-files/
/old/
/archive/
/archives/
/dump/
/dumps/
/db/
/database/
/sql/
/data/
/_backup/
/backup_YYYYMMDD/
```

**Automated Directory Fuzzing** (After finding one backup):

```bash
# If you found: https://target.com/backup/database.sql
# Fuzz for additional files in same directory

ffuf -u https://target.com/backup/FUZZ \
     -w backup-files-wordlist.txt \
     -mc 200,301,302 \
     -o backup-results.json
```


#### Phase 4: Temporal Analysis

**Date-Based Backup Discovery**:

```bash
# Generate date-based URLs for last 12 months
for month in {01..12}; do
    for day in {01..31}; do
        echo "site:target.com inurl:backup-2024-$month-$day"
        echo "site:target.com inurl:2024$month$day"
    done
done
```

**Automated Date Range Script**:

```python
from datetime import datetime, timedelta

def generate_date_dorks(target, months=12):
    today = datetime.now()
    dorks = []
    
    for i in range(months * 31):  # Approximate days
        date = today - timedelta(days=i)
        # YYYYMMDD format
        dorks.append(f'site:{target} inurl:{date.strftime("%Y%m%d")}')
        # YYYY-MM-DD format
        dorks.append(f'site:{target} inurl:{date.strftime("%Y-%m-%d")}')
    
    return dorks
```


#### Phase 5: Wayback Machine Cross-Reference

**Historical Backup Discovery**:

```bash
# Check Internet Archive for historical backups
curl "http://web.archive.org/cdx/search/cdx?url=target.com/*backup*&output=json" \
  | jq -r '.[] | . + " " + .' \
  | grep "backup"

# Look for deleted backup files
waybackurls target.com | grep -E "\.(backup|bak|old|sql|dump)$"
```

**Value**: Backups may have been removed from live site but still exist in archives or on server

***

## üõ°Ô∏è Remediation Recommendations: Comprehensive Security Controls

### Critical Priority (Immediate Action Required)

#### Finding: Publicly Accessible Database Backup File

**CVSS 3.1 Score**: 9.1 (Critical)

- **AV:N** (Network) - Accessible via internet
- **AC:L** (Low) - No special conditions required
- **PR:N** (None) - No authentication needed
- **UI:N** (None) - No user interaction required
- **S:U** (Unchanged) - Impact limited to target
- **C:H** (High) - Complete data confidentiality loss
- **I:H** (High) - Potential data modification via cracked credentials
- **A:N** (None) - No direct availability impact

**CWE**: CWE-530 (Exposure of Backup File to an Unauthorized Control Sphere)

***

### Remediation Step 1: Immediate File Removal and Access Restriction

**Emergency Actions**:

```bash
# 1. Immediately remove publicly accessible backup file
rm /var/www/html/backup/database.backup

# 2. Remove entire backup directory from webroot
rm -rf /var/www/html/backup/
rm -rf /var/www/html/backups/
rm -rf /var/www/html/old/

# 3. Verify removal
ls -la /var/www/html/ | grep -E "(backup|old|archive)"

# 4. Check for additional backup files
find /var/www/html/ -type f \( -name "*.backup" -o -name "*.bak" -o -name "*.old" -o -name "*.sql" \)
```

**Web Server Configuration - Block Backup File Extensions**:

**Apache (.htaccess or httpd.conf)**:

```apache
# Block access to backup files
<FilesMatch "\.(backup|bak|old|sql|dump|tar|gz|zip|7z|swp)$">
    Require all denied
</FilesMatch>

# Alternative: Return 404 instead of 403
<FilesMatch "\.(backup|bak|old|sql|dump)$">
    RedirectMatch 404 ^.*$
</FilesMatch>
```

**Nginx (nginx.conf)**:

```nginx
location ~ \.(backup|bak|old|sql|dump|tar|gz|zip|7z|swp)$ {
    deny all;
    return 404;
}

# Block backup directories
location ~ ^/(backup|backups|old|archive)/ {
    deny all;
    return 404;
}
```

**IIS (web.config)**:

```xml
<system.webServer>
  <security>
    <requestFiltering>
      <fileExtensions>
        <add fileExtension=".backup" allowed="false" />
        <add fileExtension=".bak" allowed="false" />
        <add fileExtension=".old" allowed="false" />
        <add fileExtension=".sql" allowed="false" />
        <add fileExtension=".dump" allowed="false" />
      </fileExtensions>
    </requestFiltering>
  </security>
</system.webServer>
```

**Verification**:

```bash
# Test that backup files return 403/404
curl -I https://target.com/test.backup
curl -I https://target.com/backup/database.sql
curl -I https://target.com/old/site.zip

# Expected: HTTP/1.1 403 Forbidden or 404 Not Found
```


***

### Remediation Step 2: Google De-Indexing and Search Engine Removal

**Remove from Google Index**:

**Method 1: Google Search Console**

```
1. Log in to Google Search Console
2. Go to: Removals ‚Üí New Request
3. Submit URL: https://target.com/backup/database.backup
4. Select: "Temporarily remove URL from Google Search"
5. Confirm removal (effective ~24 hours)
```

**Method 2: robots.txt Disallow** (Prevention):

```
User-agent: *
Disallow: /backup/
Disallow: /backups/
Disallow: /old/
Disallow: /archive/
Disallow: /*.backup$
Disallow: /*.bak$
Disallow: /*.old$
Disallow: /*.sql$
Disallow: /*.dump$
```

**Method 3: Meta Noindex Tags** (If pages exist):

```html
<meta name="robots" content="noindex, nofollow">
```

**Method 4: X-Robots-Tag HTTP Header**:

```apache
# Apache
<FilesMatch "\.(backup|bak|old|sql)$">
    Header set X-Robots-Tag "noindex, nofollow"
</FilesMatch>
```

```nginx
# Nginx
location ~ \.(backup|bak|old|sql)$ {
    add_header X-Robots-Tag "noindex, nofollow";
}
```

**Verification**:

```bash
# Check if Google has de-indexed
site:target.com AND (ext:"backup" OR ext:"bak" OR ext:"old")

# Should return: No results found
```


***

### Remediation Step 3: Credential Rotation and User Audit

**Immediate Password Resets**:

```sql
-- 1. Connect to database as administrator
mysql -u root -p

-- 2. Force password reset for admin account
UPDATE users 
SET password_hash = NULL, 
    must_change_password = TRUE,
    password_reset_token = UUID(),
    password_reset_expiry = DATE_ADD(NOW(), INTERVAL 24 HOUR)
WHERE role = 'admin';

-- 3. Invalidate all existing sessions
DELETE FROM user_sessions WHERE user_id IN (
    SELECT id FROM users WHERE role = 'admin'
);

-- 4. Log security event
INSERT INTO security_audit_log (event_type, description, timestamp)
VALUES ('SECURITY_INCIDENT', 'Forced password reset due to backup exposure', NOW());

-- 5. Notify admin via email (application-level trigger)
```

**Application-Level Password Reset**:

```bash
# Send password reset emails to all admins
php artisan tinker
>>> User::where('role', 'admin')->each(function($user) {
        $user->sendPasswordResetNotification($user->createResetToken());
    });
```

**User Account Audit**:

```sql
-- Check for suspicious accounts created recently
SELECT id, username, email, role, created_at, last_login
FROM users
WHERE created_at > DATE_SUB(NOW(), INTERVAL 30 DAY)
AND role IN ('admin', 'moderator')
ORDER BY created_at DESC;

-- Identify inactive admin accounts
SELECT id, username, email, last_login
FROM users
WHERE role = 'admin'
AND (last_login < DATE_SUB(NOW(), INTERVAL 90 DAY) OR last_login IS NULL);

-- Audit permission changes
SELECT * FROM user_role_changes
WHERE changed_at > 'discovered_exposure_date'
ORDER BY changed_at DESC;
```


***

### Remediation Step 4: Secure Backup Storage Configuration

**Principle: Backups Should Never Be Web-Accessible**

**Proper Backup Location Strategy**:

```bash
# WRONG: Backup in webroot
/var/www/html/backup/database.sql   ‚ùå

# CORRECT: Backup outside webroot
/var/backups/database/backup_20241227.sql   ‚úÖ
/opt/company/backups/database.sql.gpg       ‚úÖ
```

**Automated Backup Script** (Secure Implementation):

```bash
#!/bin/bash
# /usr/local/bin/secure-db-backup.sh

# Configuration
BACKUP_DIR="/var/backups/database"
DB_NAME="production_db"
DB_USER="backup_user"
DB_PASS="stored_in_vault_or_env"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/db_${DB_NAME}_${DATE}.sql"
GPG_RECIPIENT="admin@company.com"

# Create backup directory if not exists
mkdir -p "$BACKUP_DIR"
chmod 700 "$BACKUP_DIR"  # Only root can access

# Perform database dump
mysqldump -u "$DB_USER" -p"$DB_PASS" "$DB_NAME" > "$BACKUP_FILE"

# Encrypt backup with GPG
gpg --encrypt --recipient "$GPG_RECIPIENT" "$BACKUP_FILE"
rm "$BACKUP_FILE"  # Remove unencrypted backup

# Set restrictive permissions
chmod 600 "$BACKUP_FILE.gpg"
chown root:root "$BACKUP_FILE.gpg"

# Optional: Upload to secure cloud storage (S3 with encryption)
aws s3 cp "$BACKUP_FILE.gpg" \
    s3://company-backups-encrypted/databases/ \
    --sse aws:kms \
    --sse-kms-key-id "arn:aws:kms:region:account:key/key-id"

# Remove local backup after cloud upload (optional)
# rm "$BACKUP_FILE.gpg"

# Rotate old backups (keep last 30 days)
find "$BACKUP_DIR" -type f -name "*.sql.gpg" -mtime +30 -delete

# Log backup completion
logger "Database backup completed: $BACKUP_FILE.gpg"
```

**Scheduled Backup with Cron**:

```bash
# /etc/cron.d/database-backup
# Daily backup at 2 AM
0 2 * * * root /usr/local/bin/secure-db-backup.sh >> /var/log/backup.log 2>&1
```


***

### Remediation Step 5: Implement Strong Password Hashing

**Current Issue**: bcrypt with cost factor 10 is moderate; upgrade for better security

**Recommended Configuration**:

**PHP (bcrypt with higher cost)**:

```php
<?php
// Upgrade from cost 10 to cost 12
$options = [
    'cost' => 12,  // 2^12 = 4,096 iterations (4x slower to crack)
];
$hash = password_hash($password, PASSWORD_BCRYPT, $options);

// Even better: Use Argon2id (winner of Password Hashing Competition)
$options = [
    'memory_cost' => 65536,  // 64 MB
    'time_cost' => 4,        // 4 iterations
    'threads' => 3,          // Parallel threads
];
$hash = password_hash($password, PASSWORD_ARGON2ID, $options);
?>
```

**Python (Django)**:

```python
# settings.py
PASSWORD_HASHERS = [
    'django.contrib.auth.hashers.Argon2PasswordHasher',
    'django.contrib.auth.hashers.PBKDF2PasswordHasher',
    'django.contrib.auth.hashers.BCryptSHA256PasswordHasher',
]

# Argon2 configuration
ARGON2_TIME_COST = 4
ARGON2_MEMORY_COST = 65536
ARGON2_PARALLELISM = 3
```

**Node.js (bcrypt)**:

```javascript
const bcrypt = require('bcrypt');
const saltRounds = 12;  // Upgrade from 10 to 12

// Hash password
const hash = await bcrypt.hash(password, saltRounds);

// Or use Argon2
const argon2 = require('argon2');
const hash = await argon2.hash(password, {
    type: argon2.argon2id,
    memoryCost: 65536,
    timeCost: 4,
    parallelism: 3
});
```

**Gradual Migration Strategy**:

```php
// Detect old hashes and upgrade on next login
function verifyAndUpgradePassword($user, $password) {
    // Verify current hash
    if (password_verify($password, $user->password_hash)) {
        
        // Check if hash needs upgrade
        if (password_needs_rehash($user->password_hash, PASSWORD_ARGON2ID)) {
            // Rehash with stronger algorithm
            $newHash = password_hash($password, PASSWORD_ARGON2ID);
            
            // Update database
            DB::table('users')
                ->where('id', $user->id)
                ->update(['password_hash' => $newHash]);
        }
        
        return true;
    }
    return false;
}
```


***

### Remediation Step 6: Implement Backup Integrity Monitoring

**Automated Backup Exposure Detection**:

```python
#!/usr/bin/env python3
# /usr/local/bin/backup-exposure-monitor.py

import requests
from googlesearch import search
import smtplib
from email.mime.text import MIMEText
import time

class BackupExposureMonitor:
    def __init__(self, domain):
        self.domain = domain
        self.backup_extensions = ['backup', 'bak', 'old', 'sql', 'dump', 'zip', 'tar.gz']
        self.alert_email = 'security-team@company.com'
    
    def scan_google(self):
        """Scan Google for exposed backups"""
        findings = []
        
        for ext in self.backup_extensions:
            dork = f'site:{self.domain} ext:{ext}'
            print(f"[*] Scanning: {dork}")
            
            try:
                results = list(search(dork, num_results=10))
                if results:
                    findings.append({
                        'extension': ext,
                        'dork': dork,
                        'urls': results
                    })
                time.sleep(15)  # Rate limiting
            except Exception as e:
                print(f"[!] Error: {e}")
        
        return findings
    
    def verify_accessibility(self, url):
        """Check if file is actually accessible"""
        try:
            response = requests.head(url, timeout=10, allow_redirects=True)
            return response.status_code == 200
        except:
            return False
    
    def send_alert(self, findings):
        """Send email alert to security team"""
        subject = f"[CRITICAL] Backup Files Exposed on {self.domain}"
        body = f"Exposed backup files detected:\n\n"
        
        for finding in findings:
            body += f"\nExtension: {finding['extension']}\n"
            body += f"Dork: {finding['dork']}\n"
            body += "URLs:\n"
            for url in finding['urls']:
                accessible = self.verify_accessibility(url)
                status = "ACCESSIBLE" if accessible else "REMOVED"
                body += f"  - {url} [{status}]\n"
        
        msg = MIMEText(body)
        msg['Subject'] = subject
        msg['From'] = 'security-monitor@company.com'
        msg['To'] = self.alert_email
        
        # Send email (configure SMTP settings)
        # smtp = smtplib.SMTP('smtp.company.com', 587)
        # smtp.send_message(msg)
        print(body)  # For testing
    
    def run(self):
        """Execute monitoring scan"""
        print(f"[+] Starting backup exposure scan for {self.domain}")
        findings = self.scan_google()
        
        if findings:
            print(f"[!] WARNING: {len(findings)} types of backup files found!")
            self.send_alert(findings)
        else:
            print("[+] No exposed backups detected")

# Schedule with cron: 0 0 * * 0 (Weekly on Sunday)
if __name__ == "__main__":
    monitor = BackupExposureMonitor("target.com")
    monitor.run()
```

**Cron Schedule**:

```bash
# /etc/cron.d/backup-exposure-monitor
# Weekly scan every Sunday at midnight
0 0 * * 0 root /usr/bin/python3 /usr/local/bin/backup-exposure-monitor.py
```


***

## üîê Long-Term Security Hardening

### 1. File System Permissions Hardening

**Restrictive Permissions on Backup Directories**:

```bash
# Backup directory should be outside webroot
mkdir -p /var/backups/company
chmod 700 /var/backups/company
chown backup-user:backup-group /var/backups/company

# Individual backup files
chmod 600 /var/backups/company/*.sql.gpg
chown backup-user:backup-group /var/backups/company/*.sql.gpg

# Verify permissions
ls -la /var/backups/company/

# Expected:
# drwx------ 2 backup-user backup-group  4096 Dec 27 14:32 .
# -rw------- 1 backup-user backup-group 52428 Dec 27 14:32 backup.sql.gpg
```


### 2. Web Application Firewall (WAF) Rules

**ModSecurity Rules** (Apache/Nginx with ModSecurity):

```apache
# Block requests for backup file extensions
SecRule REQUEST_URI "@rx \.(backup|bak|old|sql|dump|tar|gz|zip|swp)$" \
    "id:1003,phase:1,deny,status:404,msg:'Backup File Access Attempt'"

# Block backup directory access
SecRule REQUEST_URI "@rx /(backup|backups|old|archive)/" \
    "id:1004,phase:1,deny,status:404,msg:'Backup Directory Access Attempt'"

# Block editor temporary files
SecRule REQUEST_URI "@rx ~$" \
    "id:1005,phase:1,deny,status:404,msg:'Editor Backup File Access Attempt'"
```

**AWS WAF Rule**:

```json
{
  "Name": "BlockBackupFileAccess",
  "Priority": 2,
  "Statement": {
    "ByteMatchStatement": {
      "FieldToMatch": {
        "UriPath": {}
      },
      "TextTransformations": [
        {
          "Priority": 0,
          "Type": "LOWERCASE"
        }
      ],
      "PositionalConstraint": "ENDS_WITH",
      "SearchString": ".backup"
    }
  },
  "Action": {
    "Block": {
      "CustomResponse": {
        "ResponseCode": 404
      }
    }
  }
}
```


### 3. Continuous Security Monitoring

**SIEM Integration**:

```bash
# Monitor for backup file access attempts in logs
tail -f /var/log/apache2/access.log | grep -E "\.(backup|bak|old|sql|dump)"

# Alert on suspicious patterns
grep -E "GET.*\.(backup|bak|old|sql)" /var/log/nginx/access.log | \
  while read line; do
    echo "[ALERT] Backup file access attempt: $line" | \
    mail -s "Security Alert" security-team@company.com
  done
```

**Automated Alert Script**:

```python
# Real-time log monitoring
import time
import re
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class BackupAccessMonitor(FileSystemEventHandler):
    def __init__(self, log_file):
        self.log_file = log_file
        self.backup_pattern = re.compile(r'\.(backup|bak|old|sql|dump)')
    
    def on_modified(self, event):
        if event.src_path == self.log_file:
            with open(self.log_file, 'r') as f:
                # Read last line
                lines = f.readlines()
                if lines:
                    last_line = lines[-1]
                    if self.backup_pattern.search(last_line):
                        self.send_alert(last_line)
    
    def send_alert(self, log_entry):
        print(f"[ALERT] Backup file access detected: {log_entry}")
        # Send to SIEM, email, Slack, etc.

# Usage
observer = Observer()
monitor = BackupAccessMonitor('/var/log/nginx/access.log')
observer.schedule(monitor, path='/var/log/nginx/', recursive=False)
observer.start()
```


### 4. Backup Encryption Best Practices

**GPG Encryption for Backups**:

```bash
# Generate GPG key for backup encryption
gpg --full-generate-key

# Export public key for backup server
gpg --export -a "Backup Admin" > backup-admin-public.key

# Encrypt backup
gpg --encrypt --recipient "Backup Admin" database-backup.sql

# Verify encryption
file database-backup.sql.gpg
# Output: GPG encrypted data

# Decrypt when needed (requires private key)
gpg --decrypt database-backup.sql.gpg > database-backup.sql
```

**AES Encryption Alternative**:

```bash
# Encrypt with AES-256
openssl enc -aes-256-cbc -salt -pbkdf2 \
  -in database-backup.sql \
  -out database-backup.sql.enc \
  -k "strong-encryption-password"

# Decrypt
openssl enc -aes-256-cbc -d -pbkdf2 \
  -in database-backup.sql.enc \
  -out database-backup.sql \
  -k "strong-encryption-password"
```


### 5. Backup Retention and Rotation Policy

**Automated Rotation Script**:

```bash
#!/bin/bash
# /usr/local/bin/backup-rotation.sh

BACKUP_DIR="/var/backups/database"

# Keep daily backups for 7 days
find "$BACKUP_DIR" -type f -name "daily-*.sql.gpg" -mtime +7 -delete

# Keep weekly backups for 30 days
find "$BACKUP_DIR" -type f -name "weekly-*.sql.gpg" -mtime +30 -delete

# Keep monthly backups for 365 days
find "$BACKUP_DIR" -type f -name "monthly-*.sql.gpg" -mtime +365 -delete

# Log rotation activity
echo "$(date): Backup rotation completed" >> /var/log/backup-rotation.log
```


***

## üìä Pentester's Backup File Discovery Checklist

**Reconnaissance Phase**:

- [ ] Enumerate all subdomains and check for backup-related names
- [ ] Identify backup file extensions used by target technology stack
- [ ] Research common backup naming conventions for target CMS/framework
- [ ] Check Wayback Machine for historical backup files

**Execution Phase**:

- [ ] Execute primary backup extension queries (backup, bak, old)
- [ ] Test database-specific backup extensions (sql, dump, mysql, pgsql)
- [ ] Search for archive formats (zip, tar.gz, 7z)
- [ ] Hunt for editor backup artifacts (swp, ~)
- [ ] Look for version control exposures (.git, .svn)
- [ ] Check cloud storage buckets (S3, Azure, GCS)
- [ ] Test backup directory patterns (/backup/, /backups/, /old/)

**Analysis Phase**:

- [ ] Download all discovered backup files
- [ ] Verify file types and actual contents
- [ ] Extract password hashes from database dumps
- [ ] Identify hash algorithms and cracking difficulty
- [ ] Extract configuration data and credentials
- [ ] Map database schema for SQL injection planning
- [ ] Document sensitive PII and regulatory data

**Exploitation Phase**:

- [ ] Set up Hashcat with appropriate hash mode
- [ ] Execute quick wins with rockyou.txt
- [ ] Build custom wordlists from reconnaissance
- [ ] Perform mask attacks based on password policy
- [ ] Test cracked credentials on admin panels
- [ ] Check for credential reuse across services
- [ ] Assess administrative access capabilities

**Reporting Phase**:

- [ ] Document all discovered backup files with URLs
- [ ] Calculate CVSS scores for findings
- [ ] Provide evidence (screenshots, extracted samples)
- [ ] Demonstrate exploitation path (discovery ‚Üí download ‚Üí crack ‚Üí access)
- [ ] Write detailed remediation steps
- [ ] Include prevention measures and security controls

***

## üß∞ Tools for Automated Backup Discovery

### 1. Custom Multi-Extension Scanner

```python
#!/usr/bin/env python3
# backup-hunter.py

import requests
from googlesearch import search
import time
import json
from urllib.parse import urlparse
import hashlib

class BackupHunter:
    def __init__(self, target_domain):
        self.domain = target_domain
        self.findings = []
        
        # Comprehensive extension list
        self.extensions = [
            # Generic backups
            'backup', 'bak', 'old', 'orig', 'copy', 'save',
            # Database specific
            'sql', 'dump', 'mysql', 'pgsql', 'db', 'sqlite',
            # Archives
            'zip', 'tar', 'tar.gz', 'tgz', 'tar.bz2', '7z', 'rar',
            # Editor artifacts
            'swp', 'swo', '~',
            # Code backups
            'php.bak', 'asp.old', 'aspx.bak',
        ]
        
        # Common backup directories
        self.directories = [
            'backup', 'backups', 'old', 'archive', 'archives',
            'dump', 'dumps', 'db', 'database', 'data', '_backup'
        ]
    
    def scan_extensions(self):
        """Scan for files with backup extensions"""
        print(f"[*] Scanning {self.domain} for backup extensions...")
        
        for ext in self.extensions:
            dork = f'site:{self.domain} ext:{ext}'
            print(f"  [*] Testing: {dork}")
            
            try:
                results = list(search(dork, num_results=10))
                if results:
                    for url in results:
                        self.findings.append({
                            'type': 'extension',
                            'extension': ext,
                            'url': url,
                            'accessible': self.check_accessibility(url)
                        })
                time.sleep(12)  # Rate limiting
            except Exception as e:
                print(f"  [!] Error: {e}")
    
    def scan_directories(self):
        """Scan for backup directories"""
        print(f"\n[*] Scanning for backup directories...")
        
        for directory in self.directories:
            dork = f'site:{self.domain} inurl:{directory}'
            print(f"  [*] Testing: {dork}")
            
            try:
                results = list(search(dork, num_results=5))
                if results:
                    for url in results:
                        self.findings.append({
                            'type': 'directory',
                            'directory': directory,
                            'url': url,
                            'accessible': self.check_accessibility(url)
                        })
                time.sleep(12)
            except Exception as e:
                print(f"  [!] Error: {e}")
    
    def check_accessibility(self, url):
        """Verify if URL is accessible"""
        try:
            response = requests.head(url, timeout=10, allow_redirects=True)
            return {
                'status_code': response.status_code,
                'accessible': response.status_code == 200,
                'size': response.headers.get('Content-Length', 'Unknown')
            }
        except Exception as e:
            return {
                'status_code': None,
                'accessible': False,
                'error': str(e)
            }
    
    def download_and_analyze(self, url):
        """Download backup file and extract metadata"""
        try:
            response = requests.get(url, timeout=30)
            if response.status_code == 200:
                content = response.content
                return {
                    'size': len(content),
                    'md5': hashlib.md5(content).hexdigest(),
                    'sha256': hashlib.sha256(content).hexdigest(),
                    'preview': content[:500].decode('utf-8', errors='ignore')
                }
        except Exception as e:
            return {'error': str(e)}
    
    def generate_report(self):
        """Generate JSON report"""
        accessible_findings = [f for f in self.findings if f.get('accessible', {}).get('accessible')]
        
        report = {
            'target': self.domain,
            'scan_date': time.strftime("%Y-%m-%d %H:%M:%S"),
            'total_findings': len(self.findings),
            'accessible_findings': len(accessible_findings),
            'findings': self.findings
        }
        
        filename = f"{self.domain}_backup_scan_{time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\n[+] Report saved: {filename}")
        print(f"[+] Total findings: {len(self.findings)}")
        print(f"[+] Accessible files: {len(accessible_findings)}")
        
        return report
    
    def run(self):
        """Execute complete scan"""
        self.scan_extensions()
        self.scan_directories()
        return self.generate_report()

# Usage
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 backup-hunter.py <target_domain>")
        sys.exit(1)
    
    hunter = BackupHunter(sys.argv)
    hunter.run()
```

**Execution**:

```bash
python3 backup-hunter.py target.com
```


### 2. Integration with Existing Tools

**Amass + Backup Dorking**:

```bash
# Enumerate subdomains first
amass enum -passive -d target.com -o subdomains.txt

# Dork each subdomain for backups
while read subdomain; do
    echo "[*] Checking $subdomain"
    echo "site:$subdomain (ext:backup OR ext:bak OR ext:old OR ext:sql)"
done < subdomains.txt > dorks.txt

# Execute dorks (manual or automated)
```

**Nuclei Template for Backup Detection**:

```yaml
# backup-files.yaml
id: backup-file-exposure

info:
  name: Backup File Exposure Detection
  author: pentester
  severity: high
  description: Detects publicly accessible backup files

http:
  - method: GET
    path:
      - "{{BaseURL}}/backup/database.sql"
      - "{{BaseURL}}/backup/site.zip"
      - "{{BaseURL}}/database.backup"
      - "{{BaseURL}}/backup.sql"
      - "{{BaseURL}}/db.sql"
      - "{{BaseURL}}/old/database.sql"
    
    matchers-condition: and
    matchers:
      - type: status
        status:
          - 200
      
      - type: word
        words:
          - "INSERT INTO"
          - "CREATE TABLE"
          - "mysqldump"
        condition: or
    
    extractors:
      - type: regex
        regex:
          - "password.*?['\"](.*?)['\"]"
          - "INSERT INTO (.*?) VALUES"
```

**Usage**:

```bash
nuclei -u https://target.com -t backup-files.yaml
```


***

## üìö Additional Resources

| **Resource** | **Description** | **URL** |
| :-- | :-- | :-- |
| OWASP Testing Guide | Backup File Testing Methodology | owasp.org/www-project-web-security-testing-guide/  |
| CWE-530 | Exposure of Backup File | cwe.mitre.org/data/definitions/530.html |
| Hashcat Wiki | Password Cracking Guide | hashcat.net/wiki/  |
| TCM Security | Password Cracking Methodology | tcm-sec.com/password-cracking-for-pentesters  |
| GitHub GHDB | Backup File Dorks | github.com/readloud/Google-Hacking-Database  |


***

## üéì Key Takeaways for Penetration Testers

### Why Backup File Discovery is Critical

1. **High Value, Low Effort**: Single Google query can expose entire database
2. **Complete Data Breach**: Backup files contain comprehensive datasets
3. **Offline Analysis**: Downloaded files can be analyzed without detection
4. **Credential Extraction**: Password hashes enable administrative access
5. **Common Misconfiguration**: Frequently found in real-world assessments

### How to Use in Engagements

**Black Box Testing**:

- Always include backup file enumeration in initial OSINT phase
- Download and analyze all discovered backup files offline
- Prioritize database dumps for credential extraction

**Bug Bounty Hunting**:

- Quick wins for sensitive data exposure bounties
- Combine with subdomain enumeration for maximum coverage
- High severity findings with clear exploitation path

**Red Team Operations**:

- Mimics real attacker behavior (passive reconnaissance)
- Provides initial access credentials for persistence
- Enables understanding of internal architecture

**Compliance Assessments**:

- Demonstrates severe data protection failures
- Proves GDPR, PCI DSS, HIPAA violations
- Highlights inadequate backup security practices


### What Makes a Strong Finding

Your penetration test report should include:

1. **Clear Vulnerability Description**: Publicly accessible backup file with sensitive data
2. **Discovery Methodology**: Exact Google Dork query used
3. **Evidence**: Screenshots and hash samples (not full data)
4. **Exploitation Demonstration**: Password cracking results (without revealing actual passwords)
5. **Impact Assessment**: Data breach potential, compliance violations, financial risk
6. **CVSS Scoring**: Quantified risk (typically 8.5-9.5 Critical)
7. **Detailed Remediation**: Step-by-step secure backup implementation

***

## ‚öñÔ∏è Ethical and Legal Considerations

**Authorization Requirements**:

- Google Dorking public information is generally legal
- Downloading discovered files may require explicit authorization
- Password cracking requires written permission in engagement scope
- Never exfiltrate actual customer data beyond proof-of-concept samples

**Responsible Disclosure**:

- Notify client immediately of backup exposure
- Recommend emergency removal of publicly accessible backups
- Provide secure data handling guidance
- Follow coordinated disclosure timelines

**Data Handling**:

- Encrypt any downloaded backup files immediately
- Delete all client data upon engagement completion
- Use anonymized samples in reports
- Follow data retention policies in contract

**Professional Standards**:

- Test only authorized systems and domains
- Document all activities for audit trail
- Maintain client confidentiality
- Adhere to industry certifications (OSCP, CEH, PNPT)

***

## üèÅ Conclusion

This demonstration illustrates the critical security risk posed by publicly accessible backup files. A simple Google Dork query successfully identified a database backup containing administrative credentials in hash form. Through offline password cracking with Hashcat, these hashes can be converted to plaintext passwords, enabling complete application compromise.

For penetration testers, backup file discovery through Google Dorking provides:

- **Efficiency**: Rapid discovery of high-severity vulnerabilities
- **Stealth**: Passive reconnaissance with no target interaction
- **Completeness**: Entire application datasets and architecture revealed
- **Exploitability**: Direct path from discovery to credential access

The systematic methodology documented in this writeup‚Äîfrom extension-based dorking through hash cracking‚Äîshould be incorporated into every web application security assessment. Backup files represent a persistent and frequently exploited vulnerability class that organizations must address through proper storage location, encryption, access controls, and continuous monitoring.

***


## üìù About This Documentation

**Practical Testing Performed By**: Muhammad Izaz Haider

**Course**: EC-Council's Short Course - *Web Application Security Testing with Google Hacking*

**Purpose**: This documentation demonstrates the practical application of Google Dorking techniques learned during the course, transforming theoretical knowledge into hands-on security testing methodologies.

---

*This writeup is intended for educational purposes and authorized security testing only. Unauthorized testing of systems you don't own or have explicit permission to test is illegal.*
